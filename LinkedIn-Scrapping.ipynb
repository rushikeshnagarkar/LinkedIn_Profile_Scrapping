{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397dd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from parsel import Selector\n",
    "from time import sleep\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import tkinter as tk\n",
    "\n",
    "# Set up the UI\n",
    "root = tk.Tk()\n",
    "root.title(\"LinkedIn Scraper\")\n",
    "root.geometry(\"400x200\")\n",
    "\n",
    "# Function to scrape LinkedIn\n",
    "def scrape_linkedin():\n",
    "    # Set up the webdriver\n",
    "    opts = Options()\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # function to ensure all key data fields have a value\n",
    "    def validate_field(field):\n",
    "        # if field is present pass\n",
    "        if field:\n",
    "            pass\n",
    "        # if field is not present print text\n",
    "        else:\n",
    "            field = 'No results'\n",
    "        return field\n",
    "\n",
    "    # Log in to LinkedIn\n",
    "    driver.get('https://www.linkedin.com')\n",
    "    sleep(10)\n",
    "    username = driver.find_element(By.ID,'session_key')\n",
    "    username.send_keys(email_entry.get())\n",
    "    sleep(0.5)\n",
    "    password = driver.find_element(By.ID,'session_password')\n",
    "    password.send_keys(password_entry.get())\n",
    "    sleep(0.5)\n",
    "    sign_in_button = driver.find_element(By.XPATH,'//*[@type=\"submit\"]')\n",
    "    sign_in_button.click()\n",
    "    sleep(15)\n",
    "\n",
    "    # Get user input for company and location\n",
    "    Companies = company_entry.get()\n",
    "    Location = location_entry.get()\n",
    "\n",
    "    # Scrape LinkedIn\n",
    "    Jobdata = []\n",
    "    lnks = []\n",
    "    driver.get(f'https://www.google.com/search?q=site%3Awww.linkedin.com%2Fin+{Companies}+%22{Location}%22')\n",
    "    time.sleep(random.uniform(2.5, 4.9))\n",
    "\n",
    "    for _ in range(1):\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"botstuff\"]/div/div/div/a/h3/div/span/span')))\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"botstuff\"]/div/div/div/a/h3/div/span/span').click()\n",
    "            time.sleep(random.uniform(2.5, 4.9))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        time.sleep(5)\n",
    "\n",
    "        linkedin_urls = [my_elem.get_attribute(\"href\") for my_elem in WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.XPATH, \"//div[@class='yuRUbf']/div/span/a[@href]\")))]\n",
    "        lnks.append(linkedin_urls)\n",
    "\n",
    "        for x in lnks:\n",
    "            for i in x:\n",
    "                driver.get(i)\n",
    "                time.sleep(random.uniform(2.5, 4.9))\n",
    "                sel = Selector(text=driver.page_source)\n",
    "                name = sel.xpath('//*[starts-with(@class, \"text-heading-xlarge inline t-24 v-align-middle break-words\")]/text()').extract_first()\n",
    "                if name:\n",
    "                    name = name.strip()\n",
    "                job_title = sel.xpath('//*[starts-with(@class, \"text-body-medium break-words\")]/text()').extract_first()\n",
    "                if job_title:\n",
    "                    job_title = job_title.strip()\n",
    "                try:\n",
    "                    company = driver.find_element(By.XPATH,'//ul[@class=\"pv-text-details__right-panel\"]').text\n",
    "                except:\n",
    "                    company = 'None'\n",
    "                if company:\n",
    "                    company = company.strip()\n",
    "                location = sel.xpath('//*[starts-with(@class, \"text-body-small inline t-black--light break-words\")]/text()').extract_first()\n",
    "                if location:\n",
    "                    location = location.strip()\n",
    "\n",
    "                driver.get(i+\"/overlay/contact-info/\")\n",
    "                time.sleep(random.uniform(2.5, 4.9))\n",
    "                lst = []\n",
    "                elems = driver.find_elements(By.XPATH,\"//a[@href]\")\n",
    "                for elem in elems:\n",
    "                    cntct = elem.get_attribute(\"href\")\n",
    "                    if cntct.endswith('?nis=true') and cntct.endswith('?nis=true&'):\n",
    "                        pass\n",
    "                    else:\n",
    "                        lst.append(cntct)\n",
    "\n",
    "                print('\\n')\n",
    "                print('Name: ' + name)\n",
    "                print('Job Title: ' + job_title)\n",
    "                print('Company: '+ company)\n",
    "                print('Location: ' + location)\n",
    "                print('URL: ' + i)\n",
    "                lst3 = []\n",
    "                for m in set(lst[:4]):\n",
    "                    lst3.append(m)\n",
    "                print(\"Contact Info\" , lst3)\n",
    "                print('\\n')\n",
    "\n",
    "                data = {\n",
    "                    'Name': name,\n",
    "                    'Job Title' : job_title,\n",
    "                    'Company': company,\n",
    "                    'Location' : location,\n",
    "                    'URL': i,\n",
    "                    'Contact_Info': lst3\n",
    "                }\n",
    "                Jobdata.append(data)\n",
    "\n",
    "    df = pd.DataFrame(Jobdata)\n",
    "\n",
    "    # Keep only the rows that have the word \"mail\" in them\n",
    "    df = df[df.apply(lambda row: row.astype(str).str.contains('mail').any(), axis=1)]\n",
    "\n",
    "    # Write the updated dataframe to a new Excel file\n",
    "    df.to_excel('Jobdata_linkedin_Leads_filtered.xlsx')\n",
    "\n",
    "# Create labels and entry fields for UI\n",
    "email_label = tk.Label(root, text=\"Email:\")\n",
    "email_label.pack()\n",
    "email_entry = tk.Entry(root)\n",
    "email_entry.pack()\n",
    "\n",
    "password_label = tk.Label(root, text=\"Password:\")\n",
    "password_label.pack()\n",
    "password_entry = tk.Entry(root, show=\"*\")\n",
    "password_entry.pack()\n",
    "\n",
    "company_label = tk.Label(root, text=\"Company:\")\n",
    "company_label.pack()\n",
    "company_entry = tk.Entry(root)\n",
    "company_entry.pack()\n",
    "\n",
    "location_label = tk.Label(root, text=\"Location:\")\n",
    "location_label.pack()\n",
    "location_entry = tk.Entry(root)\n",
    "location_entry.pack()\n",
    "\n",
    "# Create button to start scraping\n",
    "scrape_button = tk.Button(root, text=\"Scrape LinkedIn\", command=scrape_linkedin)\n",
    "scrape_button.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
